# 문장 간 유사도 측정 - 보고서

# 1. 프로젝트 개요

---

### A. 개요

보고서나 논문 등의 정보 전달 목적의 글을 쓰다 보면 같은 말을 반복해서 쓰게 되는 경우가 많다. 중복된 문장들은 가독성을 떨어뜨리는 요인 중 하나로 글의 질을 떨어뜨린다. 이런 문장들을 발견하고 수정하기 위해 사용할 수 있는 Semantic Text Similarity(STS) 모델의 성능을 높이는 것이 이번 프로젝트의 목표이다.

### B. 데이터 세트

- 총 데이터 개수: 10,974 문장 쌍
    - Train 데이터 개수: 9,324 / Dev 데이터 개수: 550 / Test 데이터 개수: 1,100
- Label 점수: 0~5 사이의 실수
    - 5점: 두 문장의 핵심 내용이 동일하며 부가적인 내용도 동일함
    - 0점: 두 문장의 핵심 내용이 동일하지 않고 부가적인 내용에서도 공통점이 없음

### C. 환경

- 컴퓨팅: Ubuntu 18.04.5 LTS(OS), Tesla V100-PCIE-32GB(GPU)
- 협업 도구: GitHub, Slack, Zoom, Notion

# 2. 프로젝트팀 구성 및 역할

---

### A. 협업 문화

- Github에 base code를 올리고 각자 branch를 개설하여 작업
- 매일 아침 10시 데일리 스크럼을 통해 진행상황 공유
- 매일 오후 4시 피어세션을 통해 실험 결과 공유 및 추후 실험 방향 결정
- main code에 병합할만한 실험 결과가 있을 경우 피어세션에서 논의 후 merge 진행

### B. 팀 구성 및 역할

| 이름 | 역할 |
| --- | --- |
| 이동호_T5139 | Checkpoint 기능 추가, sBert 모델 및 학습 방식에 따른 성능 실험 |
| 조민우_T5200 | Oversampling 초기 구현, binning에 따른 성능 실험, DA(SMOTE, SR) 수행 |
| 조재관_T5203 | LR-Scheduler에 따른 성능 실험, MSE의 과소평가 문제 해결 실험 |
| 홍성표_T5224 | EDA 및 DA 수행, Oversampling과 ensemble 코드 작성 및 추가 |
| 홍지호_T5226 | 모델 및 loss function 탐색, 소스 코드 및 버전 관리 |

# 4. 가설 설정

---

### A. EDA (Exploratory Data Analysis)

- Label의 분포
    - Train data의 경우 `label == 0`인 데이터의 개수(약 22.4%)가 가장 많다.
    
    ![Untitled](%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%80%E1%85%A1%E1%86%AB%20%E1%84%8B%E1%85%B2%E1%84%89%E1%85%A1%E1%84%83%E1%85%A9%20%E1%84%8E%E1%85%B3%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%20-%20%E1%84%87%E1%85%A9%E1%84%80%E1%85%A9%E1%84%89%E1%85%A5%20b40b4bb8134c4a9e866ff9a5207d967e/Untitled.png)
    
    - 반면, validation data의 경우 상대적으로 고른 분포를 보여준다.
    
    ![Untitled](%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%80%E1%85%A1%E1%86%AB%20%E1%84%8B%E1%85%B2%E1%84%89%E1%85%A1%E1%84%83%E1%85%A9%20%E1%84%8E%E1%85%B3%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%20-%20%E1%84%87%E1%85%A9%E1%84%80%E1%85%A9%E1%84%89%E1%85%A5%20b40b4bb8134c4a9e866ff9a5207d967e/Untitled%201.png)
    
- Data source 종류
    - `sampled` : 실제 기록에서 추출된 데이터로 판단된다. label의 평균 값은 약 1.2이다.
    - `rtt` : Round-trip translation(RTT)의 약자이다. 한국어→영어→한국어 순으로 번역한 문장이 `sentence_2`에 들어가 있다. `rtt` 데이터 label의 평균은 약 3.5로 `sampled` 데이터에 비해 높은 label 점수를 보유하고 있다.
- 기타 내용
    - `petition-xxx` : 요청 또는 주장에 관한 내용으로 구성된다.
    - `slack-xxx`, `nsmc-xxx` : 구어체 또는 인터넷 댓글과 유사한 문장으로 구성된다.
    - 특수문자, 오타, 사람의 이름을 마스킹한 단어로 추정되는 `<PERSON>`이라는 문자가 군데군데 등장한다.

### B. EDA를 통해 도출된 가설

- 의미 없는 특수문자 및 오타는 모델의 학습을 방해할 수 있다.
    - **H1**: 의미 없는 특수문자를 제거하고 맞춤법을 교정하는 등의 데이터 전처리를 한다면 모델 성능이 좋아질 것이다.
- train 데이터의 label 분포가 0.0점에 치우친 skewed distribution으로 불균형하다.
    - **H2**: 데이터 증강(DA; Data Augmentation)을 통해 데이터 label 불균형 문제를 해결한다면 모델 성능이 좋아질 것이다.

### C. 일반적 가설

- **H3**: 최적의 모델 및 hyper-parameter를 찾는다면 모델 성능이 좋아질 것이다.
- **H4**: 앙상블 기법을 사용한다면 모델 성능이 좋아질 것이다.

# 4. 가설 검증

---

### A-1. 데이터 전처리 (H1)

- `hanspell`을 활용한 맞춤법 교정
    - train data에 대해 문장 당 평균 1개 정도의 토큰이 수정되는 것을 확인하였다.
        - `sentence_1`: 1.009개 / `sentence_2`: 0.736개
    - 수정되는 토큰의 숫자가 적어 유의미한 성능 변화를 일으키지 않을 것이라 판단하고 데이터 전처리 기법에서 제외하였다.
- 특수문자 제거
    - 문장 내에서 아무런 의미가 없을 것으로 추정되는 특수문자(예: `\\` )를 제거하였다.
- `<PERSON>`  문자 토큰화
    - 특정 사람을 지칭하는 단어이기 때문에 tokenizer와 model에서 special token으로 처리했다.

### A-2. 데이터 전처리에 따른 모델 성능 비교

- model : `snunlp/KR-ELECTRA-discriminator`
- learning_rate: 1e-5 / batch_size: 16 / loss function: MSE (L2 loss) / max_epoch: 10
- 아래 val_pearson은 3번의 모델 훈련 결과에 대한 평균이다.
    
    
    | Dataset | val_pearson |
    | --- | --- |
    | raw data | 0.9082 |
    | 특수문자 제거 | 0.9083 |
    | <PERSON> special token 추가 | 0.9090 |
- 특수문자 제거의 효과는 미미하거나 거의 없는 것처럼 보인다.
- `<PERSON>` token을 추가해준 것만으로 약 0.0008만큼 val_pearson이 증가했다.
- 단 3번의 훈련 결과의 평균이기 때문에 단정지을 수는 없지만, `<PERSON>` 토큰 추가는 모델 성능 개선에 기여하는 것으로 보인다. **따라서 이후의 모델 훈련과 실험에서는 `<PERSON>` 토큰 추가를 기본으로 하였다.**

### B-1. Data Augmentation (H2)

텍스트 데이터 증강 기법에 대해서는 Ref. [1]을 참고하였다.

- Round-trip translation (RTT)
    - `sentence_1`과 `sentence_2` 각각에 대해 RTT 처리를 한 데이터 세트를 생성하였다.
    - 단, 번역 처리되면서 문장의 의미가 달라질 여지가 있기 때문에, 문장의 의미가 바뀌더라도 점수에 영향이 거의 없을법한 (= label 점수가 낮은) 데이터에 대해서만 RTT를 적용하였다.
- Self-translation (ST)
    - `label == 5` 인 데이터가 부족한 문제를 해결하기 위해, `sentence_1 == sentence_2`인 데이터를 생성하였다.
    - 이를 위해 `train_df['binary-label'] == 0` (0 ≤ label < 2.5)인 데이터에서 `sentence_1` 문장을 복사한 뒤  `sentence_2 = sentence_1, label = 5, binary-label = 1` 인 데이터를 생성하였다.
- Swapping Sentences (SS)
    - `sentence_1`과 `sentence_2`를 서로 바꾼 데이터를 추가하였다.
    - BERT 기반의 모델은 segment embedding 및 positional encoding이 들어가기 때문에 단순히 두 문장의 위치를 바꿔주는 것만으로도 다른 데이터를 입력하는 것 같은 효과가 생길 것으로 기대된다.
- Synonym Replacement (SR)
    - `sentence_1`과 `sentence_2` 에 동일하게 들어있는 단어를 유의어로 변경한 데이터를 추가하였다.
    - 유의어 사전으로는 카이스트에서 제작한 `wordnet.pickle`을 활용하였다.
- Over Sampling
    - over-sampling은 classification problem에서 데이터가 불균형할 때 사용하는 방법이다.
    - 그러나 lable 점수는 연속형 변수라는 문제가 있기 때문에 binning을 통해 만들어진 label 구간을 명목형 변수라고 가정한 후 over-sampling 기법을 적용하였다.
    - 구간화된 데이터별 데이터 숫자를 동일하게 맞춰주었다. (sampling_strategy=’all’)
- Synthetic Minority Oversampling Technique (SMOTE)
    - 숫자 영역에서 기존과 비슷한 데이터를 생성해주는 SMOTE를 임베디드 벡터에 적용하였다.
    - SMOTE와 비슷한 기법인 Border-line SMOTE 및 ADASYN도 부가적으로 적용해보았다.

### B-2. Data Augmentation에 따른 모델 성능 비교

- model: `snunlp/KR-ELECTRA-discriminator`
- learning_rate: 1e-5 / batch_size: 16 / loss function: MSE (L2 loss) / oversampling activated
    
    
    | Dataset | val_pearson | 비고 |
    | --- | --- | --- |
    | raw data | 0.9309 |  |
    | RTT | 0.9259 |  |
    | RTT+SS | 0.9253 |  |
    | RTT+SS+ST | 0.9320 |  |
    | SS | 0.9271 |  |
    | SS+ST | 0.9305 |  |
    | Reference (raw-data) | 0.9229 | Not Oversampled |
    
    ![%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%80%E1%85%A1%E1%86%AB%20%E1%84%8B%E1%85%B2%E1%84%89%E1%85%A1%E1%84%83%E1%85%A9%20%E1%84%8E%E1%85%B3%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%20-%20%E1%84%87%E1%85%A9%E1%84%80%E1%85%A9%E1%84%89%E1%85%A5%20b40b4bb8134c4a9e866ff9a5207d967e/DA_train_loss.png](%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%80%E1%85%A1%E1%86%AB%20%E1%84%8B%E1%85%B2%E1%84%89%E1%85%A1%E1%84%83%E1%85%A9%20%E1%84%8E%E1%85%B3%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%20-%20%E1%84%87%E1%85%A9%E1%84%80%E1%85%A9%E1%84%89%E1%85%A5%20b40b4bb8134c4a9e866ff9a5207d967e/DA_train_loss.png)
    
- Raw data에 대해 Oversampling만 적용해 줘도 모델 성능이 향상되었다.
    - Label의 분포를 임의로 균등하게 만들어 주는 것의 효과가 유의미한 것으로 판단된다.
- 다른 DA 기법을 적용한 데이터의 경우 raw-data에 비해 상대적으로 낮은 train_loss를 보여준다.
    - Sample 숫자가 늘어났으므로 에폭 대비 학습량이 늘어나 train_loss가 낮아졌을 수 있다.
    - raw-data와 비교해 보았을 때 train_loss 감소치에 비해 val_pearson 감소치가 낮으므로 과적합을 의심해볼 수 있다.
    - Sample 숫자가 늘어나 raw_data에 비해 학습 시간이 오래 걸린다는 단점이 있다.
- 위 표 및 그래프에는 포함되지 않았지만 SR 및 SMOTE의 경우 val_pearson이 0.90 이하로 떨어지는 모습을 보였다.
- **현재 과제에서는 더 적은 시간으로 높은 성능을 보이는 oversampling 기법만 적용해줘도 충분할 것으로 판단된다.**

### C. 최적의 모델 및 하이퍼파라미터 탐색 (H3)

**Model**, **Loss function**, **Learning rate schedular, Batch size**, **Binning**의 최적값을 찾고자 했다.

### C-1. Model

- batch_size: 16 / learning_rate: 1e-5 / Loss function: MSE (L2 loss)
    
    
    | Model name | val_pearson | Settings | Oversampling
    (binning = 0.5) |
    | --- | --- | --- | --- |
    | klue/roberta-base | 0.8394 | Default |  |
    | klue/roberta-large | 0.9057 | batch_size = 8 / lr = 1e-6 |  |
    | snunlp/KR-ELECTRA-discriminator | 0.9082 | Default |  |
    | beomi/KcELECTRA-base | X (failed) | Default |  |
    | sentence-transformers-all-MiniLM-L6-v2 | 0.699 | Default |  |
    | snunlp-KR-SBERT-V40K-klueNLI-augSTS | 0.860 | Default |  |
    | klue/roberta-small | 0.8849 | Default | activated |
    | klue/bert-base | 0.8788 | Default | activated |
    | snunlp/KR-ELECTRA-discriminator | 0.9237 | Default | activated |
    | monologgmonologg/koelectra-base-v3-discriminator | 0.9174 | Default | activated |
- ELECTRA 기반의 모델은 MLM을 통과한 텍스트 token이 원래 문장에 있는 token인지 여부를 판별하는 discriminator (Transformer Encoder, 또는 BERT 구조) model이다. STS task에서 RoBERTa 모델보다 근소하게 더 좋은 점수를 보여주는 것으로 알려져 있다. Ref. [2]
- 한국어 데이터에 대해 사전 학습된 ELECTRA 모델 중 대표적으로 `snunlp/KR-ELECTRA-discriminator`가 있으며 현재 가장 좋은 모델 성능을 보인다.
- oversampling은 대체적으로 모든 모델에서 유의미한 성능 향상을 보인다.

### C-2. Loss function

- 세 가지 Loss function(MSE, L1, Cosine similarity)에 따른 모델 성능을 비교하였다.
- dataset: raw data / learning_rate: 1e-5 / batch_size: 16 / oversampling activated
    
    
    | Model | Loss function | val_pearson | batch_size | learning_rate |
    | --- | --- | --- | --- | --- |
    | snunlp/KR-ELECTRA-discriminator | MSE | 0.9309 |  |  |
    | snunlp/KR-ELECTRA-discriminator | L1 | 0.9066 |  |  |
    | klue/roberta-large | MSE | 0.9041 |  | 1e-6 |
    | klue/roberta-large | L1 | 0.923 |  | 1e-6 |
    | monologg/koelectra-base-v3-discriminator | MSE | 0.913 |  |  |
    | monologg/koelectra-base-v3-discriminator | L1 | 0.925 |  |  |
    | snunlp/KR-ELECTRA-discriminator | MSE | 0.9237 | 32 |  |
    | snunlp/KR-ELECTRA-discriminator | SmoothL1Loss | 0.9289 | 32 |  |
    | monologgmonologg/koelectra-base-v3-discriminator | MSE | 0.9174 | 32 |  |
    | monologgmonologg/koelectra-base-v3-discriminator | L1 | 0.9248 | 32 |  |
    | jhgan/ko-sroberta-multitask | MSE | 0.918 |  |  |
    | jhgan/ko-sroberta-multitask | CosineSimilarity | 0.864 |  |  |
- L1 loss 및 CosineSimilarity에 비해 MSE가 일반적으로 더 높은 성능을 보여주었다.
- 단, `klue/roberta-large` 모델에 대해서는 L1 loss를 사용했을 때 더 좋은 성능을 보여주었다.

### C-3. Learning rate Scheduler

- Dataset: raw data / batch_size: 32 / oversampling activated
    
    
    | Model | Loss function | Scheduler | val_pearson | lr-rate |
    | --- | --- | --- | --- | --- |
    | snunlp/KR-ELECTRA-discriminator | MSE | X | 0.9233 | 1e-5 |
    | snunlp/KR-ELECTRA-discriminator | MSE | get_linear_schedule_with_warmup | 0.9305 | 5e-5 |
    | monologg/koelectra-base-v3-discriminator | L1 | X | 0.9248 | 1e-5 |
    | monologg/koelectra-base-v3-discriminator | L1 | optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15,20], gamma=0.5) | 0.9225 | 1e-5 |
    | monologg/koelectra-base-v3-discriminator | L1 | get_linear_schedule_with_warmup | 0.9114 | 5e-5 |
- `snunlp/KR-ELECTRA-discriminator` 모델은 scheduler를 적용했을 때 성능이 향상되었다.
- `monologg/koelectra-base-v3-discriminator` 모델은 scheduler를 적용했을 때 성능이 떨어졌다.
- Sweep을 활용하여 learning rate에 따른 val_pearson 값을 비교하는 실험도 진행하였다.
    - lr이 1e-5일 때 최고 성능을 보여주었다.
    - 1e-5 보다 클 경우 성능 저하를 보였으며 3e-5 보다 클 경우 NaN 값을 자주 출력하였다.

### C-4. Batch Size

- Sweep을 활용하여 batch_size가 8, 16, 32일 때의 val_pearson을 비교하였다.
    - batch size가 16 혹은 32일 때는 유의미한 성능 차이가 나타나지 않았다.
    - batch size가 8일 때는 val_pearson이 0.01 떨어졌다.

### C-5. Binning

- label은 0.0부터 5.0까지 0.2 간격으로 총 26개가 있다.
- 가장 숫자가 많은 0을 따로 묶고 그 후로 n개씩 묶어서 범주화한 후 over sampling을 진행하였다.
- n = 1, 2, 3, 4, 5인 경우로 실험을 진행한 결과 n = 3일 때 모델 성능이 가장 좋았다.

### D. Ensemble (H4)

- 준수한 성능을 보이는 모델 및 hyper-parameter 값을 적용한 여러 개의 모델을 선별하였다.
- Soft voting 방식으로 weighted averaging 된 결과 값을 최종 label로 출력하였다.

$$
label = \sum_i\frac{p_i}{\sum_i p_i} label_i
$$

- 제출 결과 val_pearson 값이 평균 0.01 정도 상승하였다.

# 5. 최종 결과

- Training Data: Raw data + Oversampling
- Used models in ensemble
    - `monologg-koelectra-base-v3-discriminator`: val_pearson=0.926
    - `snunlp-KR-ELECTRA-discriminator`: val_pearson=0.930
    - `snunlp-KR-ELECTRA-discriminator`: val_pearson=0.931
    - `snunlp-KR-ELECTRA-discriminator`: val_pearson=0.932
    - `klue-roberta-large` : val_pearson=0.923
- Public Test Score (Pearson): 0.9305 (4위)
- 최종 Private Test Score (Pearson): 0.9346 (5위)

# 6. 자체 평가 의견

---

### 1) 잘했던 점

- 소스 코드 관리를 위해 GitHub을 적극적으로 활용하였다.
- config.json을 통해 실험 변수를 편리하게 통제하였다.
- 여러 가지 DA 방법을 적용해보았으며 Oversampling을 적용한 Raw Data가 가장 효과적임을 발견하였다.

### 2) 시도 했으나 잘되지 않았던 것들

- learning rate scheduler를 사용하여 loss 값을 최소화하려고 했지만 향상된 결과를 얻지 못했다.
- Sentence bert에 맞게 구조를 바꿔 학습을 진행하였지만 성능이 크게 향상되지 않았다.
- 유의어 변환 DA를 시도했지만 오히려 성능이 하락하였다.

### 3) 아쉬웠던 점들

- K-fold CV: 프로젝트 마지막 날인 4월 20일 기준 K-fold CV에 대한 구현은 성공했지만 시간 부족으로 관련된 실험을 진행하지는 못했다.
- hyper-parameter tuning(optimizer): AdamW를 기본으로 사용하더라도 안에 있는 $\beta_1, \beta_2$, `weight_decay`를 바꿔가면서 최적의 값을 찾을 수 있었을 것이다. 또는 다른 optimizer + scheduler를 활용할 수도 있지만 대부분의 pre-trained model이 Adam 또는 AdamW + warm-up scheduler를 사용하는 것으로 파악된다.
- 데이터 라벨 활용: 데이터의 출처와 RTT 여부에 대한 라벨을 토큰을 통해서 고려해 줄 수 있었고 이는 성능향상으로 이어질 수 있었다.
    
    

### 4) 프로젝트를 통해 배운 점 또는 시사점

- 이론으로 배운 early stopping, learning rate scheduler 등을 직접 구현하며 사용 방법을 익힐 수 있었다.
- 데이터 전처리부터 hyper-parameter tuning까지 딥러닝 대회의 전반적인 프로세스를 경험해 볼 수 있었다.
- 하나의 목표를 위해 함께 노력해가는 과정 속에서 협업에 대해서 배울 수 있었다.

# Reference

---

[1] [C. Park, K. Kim, and H. Lim, *"Optimization of Data Augmentation Techniques in Neural Machine Translation"*, Annual Conference on Human and Language Technology, 10a, 258 (2019).](https://koreascience.kr/article/CFKO201930060755841.page)

[2] [K. Clark, M.-T. Luong, Q. V. Le and C. D. Manning, *"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"*, arXiv:2003.10555.](https://arxiv.org/abs/2003.10555)